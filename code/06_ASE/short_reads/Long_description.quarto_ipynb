{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Variant Calling in ONT RNA Seq\"\n",
        "format: html\n",
        "editor: visual\n",
        "execute:\n",
        "  eval: false\n",
        "---\n",
        "\n",
        "\n",
        "Variant calling works best with WGS data that has high coverage (\\> 30x, i.e. 15x per haplotype in the human genome). It can be performed with RNA-Seq data as well, but this is not recommended for reasons such as:\n",
        "\n",
        "1.  ASE: Allele-specific expression can lead to false positives of homozygous alleles present for a given gene.\n",
        "2.  Uneven coverage: Since RNA-Seq is designed to capture only the regions that are expressed in the genome, we don't get variant information on genes that are not expressed.\n",
        "\n",
        "Say that you have ONT RNA-Seq data for which you want to perform allele specific gene/isoform expression analysis. Given above, we want to perform variant calling on an orthogonal high coverage WGS data from the same sample.\n",
        "\n",
        "### Step 1:\n",
        "\n",
        "Suppose we do have this WGS data, there are two formats in which it may be available. One would be multiple FASTQ files of technical replicates of the sample. The other could be biological replicates for a given condition.\n",
        "\n",
        "In case of technical replicates, it seems to be a good idea to merge the FASTQ files into a single file and align the merged file to the reference genome using software such as `bowtie`. Merging can also be done after alignment.\n",
        "\n",
        "Example Code\n",
        "\n",
        "\n",
        "```{bash}\n",
        "\n",
        "samples=(sample_1 sample_2 sample_3) # List of samples to align\n",
        "INPUT_DIR=\"/path/to/fastq/files\" # Directory containing the FASTQ files\n",
        "OUTPUT_DIR=\"/path/to/output/sam/files\" # Directory to save the output SAM files\n",
        "ref_fa_index_dir=\"/path/to/reference/genome/index\" # Directory containing the reference genome index files\n",
        "\n",
        "for sample in ${samples[@]}\n",
        "do\n",
        "    bowtie2 -x $ref_fa_index_dir -p ${SLURM_CPUS_PER_TASK} \\\n",
        "    -1 $INPUT_DIR/${sample}_1.fastq -2 $INPUT_DIR/${sample}_2.fastq -S $OUTPUT_DIR/${sample}.sam \\\n",
        "    --rg-id ${sample} --rg \"SM:${sample}\" --rg \"PL:ILLUMINA\" \n",
        "    echo \"**** done aligning $sample ****\"\n",
        "done\n",
        "```\n",
        "\n",
        "\n",
        "### Step 2: Bam Quality Control\n",
        "\n",
        "One alignment is done, we can use `samtools` to sort the BAM file and index it. If samples are technical replicates, we can merge the BAM files using `samtools merge` command. This will create a single BAM file containing all the reads from all the samples.\n",
        "\n",
        "\n",
        "```{bash}\n",
        "samples=(sample_1 sample_2 sample_3) # List of samples to merge\n",
        "INPUT_DIR=\"/path/to/sam/files\" # Directory containing the SAM files\n",
        "OUTPUT_DIR=\"/path/to/output/bam/files\" # Directory to save the output BAM files\n",
        "for sample in ${samples[@]}\n",
        "do\n",
        "    samtools sort -@ ${SLURM_CPUS_PER_TASK} -o $OUTPUT_DIR/${sample}.sorted.bam $INPUT_DIR/${sample}.sam\n",
        "    samtools index $OUTPUT_DIR/${sample}.sorted.bam\n",
        "    echo \"**** done sorting and indexing $sample ****\"\n",
        "done\n",
        "\n",
        "samtools merge -@ ${SLURM_CPUS_PER_TASK} $OUTPUT_DIR/merged.bam $OUTPUT_DIR/*.sorted.bam\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "GATK also provides a set of commands to perform quality control on the BAM file. The first step is to mark duplicates using `MarkDuplicates`. This will help in removing PCR duplicates that can lead to false positives in variant calling (If we have a bunch of duplicate reads for a given region, we may wrongly call it homozygous).\n",
        "\n",
        "\n",
        "```{bash}\n",
        "gatk MarkDuplicates \\\n",
        "    -I $OUTPUT_DIR/merged.bam \\\n",
        "    -O $OUTPUT_DIR/merged.marked.bam \\\n",
        "    -M $OUTPUT_DIR/merged.marked.metrics.txt \\\n",
        "    --CREATE_INDEX true \\\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "The next step is to perform base quality score recalibration (BQSR) using `BaseRecalibrator`. This will help in correcting the base quality scores of the reads based on the known variants in the genome.\n",
        "\n",
        "The `BaseRecalibrator` is done by refering to known variants in the genome, which can be obtained from a VCF file. This VCF file can be obtained from a previous variant calling step or from a public database such as dbSNP or the Mills and 1000G Gold standard database for INDELs.\n",
        "\n",
        "Here's how I downloaded them:\n",
        "\n",
        "\n",
        "```{bash}\n",
        "known_sites=path_to_where_you_want_to_download_the_reference_VCFs\n",
        "mkdir -p $known_sites\n",
        "cd $known_sites\n",
        "wget -N https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz\n",
        "wget -N https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz.tbi\n",
        "\n",
        "wget -N https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf\n",
        "wget -N https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf.idx\n",
        "\n",
        "wget -N https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz\n",
        "wget -N https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi\n",
        "```\n",
        "\n",
        "\n",
        "Then the GATK `BaseRecalibrator` and `ApplyBQSR` commands can be run as follows:\n",
        "\n",
        "\n",
        "```{bash}\n",
        "gatk BaseRecalibrator \\\n",
        "  -R \"$ref_fa\" \\\n",
        "  -I \"${OUTPUT_DIR}/merged.bam\" \\\n",
        "  --known-sites \"$DBSNP\" \\\n",
        "  --known-sites \"$MANDG_INDELS\" \\\n",
        "  --known-sites \"$SNPs_1000G\" \\\n",
        "  -O \"${OUTPUT_DIR}/recal_data.table\" \n",
        "\n",
        "gatk ApplyBQSR \\\n",
        "  -R \"$ref_fa\" \\\n",
        "  -I \"${OUTPUT_DIR}/merged.bam\" \\\n",
        "  --bqsr-recal-file \"${OUTPUT_DIR}/recal_data.table\" \\\n",
        "  -O \"${OUTPUT_DIR}/recal.bam\"\n",
        "```\n",
        "\n",
        "\n",
        "The final BAM QC step is to filter the BAM file using the `samtools view` command. This can be used to remove reads that are not properly paired, have low mapping quality, or are not primary alignments.\n",
        "\n",
        "\n",
        "```{bash}\n",
        "samtools view -@ 8 -b -F 2308 -q 20 \"${OUTPUT_DIR}/recal.bam\" > \"${OUTPUT_DIR}/filtered.bam\"\n",
        "```\n",
        "\n",
        "\n",
        "### Step 3: Variant Calling\n",
        "\n",
        "Once the BAM file is ready, we can use GATK's `HaplotypeCaller` to perform variant calling. This command will call variants on the BAM file and output a VCF file containing the variants.\n",
        "\n",
        "\n",
        "```{bash}\n",
        "gatk HaplotypeCaller \\\n",
        "  -R \"$ref_fa\" \\\n",
        "  -I \"${OUTPUT_DIR}/filtered.bam\" \\\n",
        "  -O \"${OUTPUT_DIR}/variants.g.vcf\" \\\n",
        "  -ERC GVCF\n",
        "```\n",
        "\n",
        "\n",
        "In the case where we have biological replicates, this command is used on each of the samples' BAM files. The VCF files can then be merged using `GATK CombineGVCFs` command to get a single VCF file containing all the variants from all the samples.\n",
        "\n",
        "The next step is to perform joint genotyping using the `GenotypeGVCFs` command. This will take the merged VCF file and output a final VCF file containing the genotypes for each sample. For the sake of phasing with `whatshap`, this step is not helpful as `whatshap` performs phasing based on only one of the samples even from a multi-sample VCF file.\n",
        "\n",
        "In order to QC these variants, we need them split into SNPs and INDELs. This is because SNPs and INDELs need to be treated differently during QC. This can be done using the `SelectVariants` command.\n",
        "\n",
        "\n",
        "```{bash}\n",
        "gatk SelectVariants \\\n",
        "  -R \"$ref_fa\" \\\n",
        "  -V \"${OUTPUT_DIR}/variants.g.vcf\" \\\n",
        "  --select-type-to-include SNP \\\n",
        "  -O \"${OUTPUT_DIR}/SNPs.vcf.gz\"\n",
        "  \n",
        "  #replace with --select-type-to-include INDEL for INDELs\n",
        "```\n",
        "\n",
        "\n",
        "The `VariantFiltration` command can then be used to filter the variants based on various criteria such as quality, depth, and allele frequency. The following filters are applied based on GATK Best Practices:\n",
        "\n",
        "The `SelectVariants` command can be used to select only the SNPs and INDELs that PASS the filters. So now we have two final files, `SNP_PASS.vcf` and `INDEL_PASS.vcf`, which can be used for downstream analysis.\n",
        "\n",
        "\n",
        "```{bash}\n",
        "\n",
        "gatk VariantFiltration \\\n",
        "  -R \"$ref_fa\" \\\n",
        "  -V \"${OUTPUT_DIR}/SNPs.vcf.gz\" \\\n",
        "   -filter \"QD < 2.0\" --filter-name \"QD2\" \\\n",
        "   -filter \"QUAL < 30.0\" --filter-name \"QUAL30\" \\\n",
        "   -filter \"SOR > 3.0\" --filter-name \"SOR3\" \\\n",
        "   -filter \"FS > 60.0\" --filter-name \"FS60\" \\\n",
        "   -filter \"MQ < 40.0\" --filter-name \"MQ40\" \\\n",
        "   #following if you have multiple samples\n",
        "   # -filter \"MQRankSum < -12.5\" --filter-name \"MQRankSum-12.5\" \\\n",
        "   # -filter \"ReadPosRankSum < -8.0\" --filter-name \"ReadPosRankSum-8\" \\\n",
        "  -O \"${OUTPUT_DIR}/SNPs_filtered.vcf.gz\"\n",
        "  \n",
        "gatk VariantFiltration \\\n",
        "  -R \"$ref_fa\" \\\n",
        "  -V \"${OUTPUT_DIR}/INDELs.vcf.gz\" \\\n",
        "   -filter \"QD < 2.0\" --filter-name \"QD2\" \\\n",
        "   -filter \"QUAL < 30.0\" --filter-name \"QUAL30\" \\\n",
        "   -filter \"FS > 200.0\" --filter-name \"FS200\" \\\n",
        "   # doens't work for single samples\n",
        "   # -filter \"ReadPosRankSum < -20.0\" --filter-name \"ReadPosRankSum-20\" \\\n",
        "  -O \"${OUTPUT_DIR}/INDELs_filtered.vcf.gz\"\n",
        "```\n",
        "\n",
        "\n",
        "Then, to only keep the `PASS` variants:\n",
        "\n",
        "\n",
        "```{bash}\n",
        "gatk SelectVariants \\\n",
        "  -R \"$ref_fa\" \\\n",
        "  -V \"${OUTPUT_DIR}/SNPs_filtered.vcf.gz\" \\\n",
        "  --exclude-filtered \\\n",
        "  -O \"${OUTPUT_DIR}/SNPs_filtered_PASS.vcf.gz\"  \n",
        "  \n",
        "gatk SelectVariants \\\n",
        "  -R \"$ref_fa\" \\\n",
        "  -V \"${OUTPUT_DIR}/INDELs_filtered.vcf.gz\" \\\n",
        "  --exclude-filtered \\\n",
        "  -O \"${OUTPUT_DIR}/INDELs_filtered_PASS.vcf.gz\"\n",
        "```\n",
        "\n",
        "\n",
        "These files can be merged using the `MergeVcfs` command to get a single VCF file containing all the variants.\n",
        "\n",
        "\n",
        "```{bash}\n",
        "gatk MergeVcfs \\\n",
        "  -I \"${OUTPUT_DIR}/SNPs_filtered_PASS.vcf.gz\" \\\n",
        "  -I \"${OUTPUT_DIR}/INDELs_filtered_PASS.vcf.gz\" \\\n",
        "  -O \"${OUTPUT_DIR}/all_variants.vcf.gz\"\n",
        "```\n",
        "\n",
        "\n",
        "### Step 4: Phasing\n",
        "\n",
        "Phasing is the process of determining which alleles are inherited together on the same chromosome. This is important for allele-specific expression analysis as it helps in determining which allele is expressed in a given gene.\n",
        "\n",
        "Phasing can be done using `whatshap` software. The `whatshap phase` command can be used to produce a phased VCF file. This command can take multiple long read BAM files and the VCF file at the end of step 3 as input and outputs a phased VCF file containing the phased variants.\n",
        "\n",
        "\n",
        "```{bash}\n",
        "\n",
        "conda activate whatshap \n",
        "\n",
        "whatshap phase -o $phased_vcf_dir/phased.vcf --reference=$ref_fa --ignore-read-groups ${OUTPUT_DIR}/all_variants.vcf.gz \\ \n",
        "# vcf from previous step\n",
        "$genome_bam_dir/input_bam1_for_phasing.bam $genome_bam_dir/input_bam2_for_phasing.bam\n",
        "# above we're providing a list of bam files that help with phasing our input VCF \n",
        "# the same bams will be haplotagged in the next step\n",
        "```\n",
        "\n",
        "\n",
        "`whatshap haplotag` is then used to tag the reads in the BAM file with the haplotype information . This command takes the phased VCF file and our long read BAM files as input and outputs a new BAM file with the haplotype information tagged as `HP1` or `HP2` if they are heterozygous. These tags can be used to `split` the BAM file into two separate BAM files, one for each haplotype.\n",
        "\n",
        "\n",
        "```{bash}\n",
        "\n",
        "$input_bam_dir=\"/path/to/long_read_bam_files\" # Directory containing the long read BAM files\n",
        "${lr_sample}=bam_1 # replace with your long read sample name\n",
        "whatshap haplotag -o $whatshap_output_dir/${lr_sample}.bam \\\n",
        "--reference $ref_fa $phased_vcf $input_bam_dir/${lr_sample}.bam \\\n",
        "--output-threads=19 --ignore-read-groups --output-haplotag-list $whatshap_output_dir/${lr_sample}_haplotypes.tsv\n",
        "\n",
        "echo \"**** Splitting haplotagged BAM into haplotypes ****\"\n",
        "whatshap split --output-h1 $whatshap_output_dir/${lr_sample}_h1.bam \\\n",
        "--output-h2 $whatshap_output_dir/${lr_sample}_h2.bam $whatshap_output_dir/${lr_sample}.bam $whatshap_output_dir/${lr_sample}_haplotypes.tsv\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "### Step 5: Allele-specific expression analysis\n",
        "\n",
        "Once we have the haplotype specific BAM file(s), we can use the `featureCounts` function from the `Rsubread` in `R` to count the reads for each gene/isoform. The `isLongRead` has to be set to `TRUE` to indicate that the input BAM file is a long read BAM file. It is unclear whether to set the `allowMultiOverlap` to `TRUE` or `FALSE`. The `featureCounts` function will output a count matrix with the counts for each gene/isoform for each haplotype.\n",
        "\n",
        "### Step 6: Differential expression analysis\n",
        "\n",
        "Once we have the count matrix, we can use popular differential expression analysis tools such as `DESeq2` or `edgeR` to perform differential expression analysis. In our counts matrix each column refers to a specific haplotype for a specific sample/cell depending on whether we're working with bulk or single cell data."
      ],
      "id": "8b8bde0b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/sparthib/Library/Python/3.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}